{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69e78ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#pip install undetected-chromedriver\n",
    "\n",
    "#import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "#service = Service(executable_path=\"/Users/sachinsrivastava/Downloads/PythonCodes/chromedriver\")\n",
    "#driver = webdriver.Chrome(service=service)\n",
    "# from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dc52f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global path\n",
    "file_path = '/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8b978d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Declare Chrome Options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_experimental_option('prefs', {\n",
    "# \"download.default_directory\": \"/Users/sachinsrivastava/Downloads/PythonCodes/AMR_Downloads\", #Change default directory for downloads\n",
    "# \"download.prompt_for_download\": False, #To auto download the file\n",
    "# \"download.directory_upgrade\": True,\n",
    "# \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "# })\n",
    "\n",
    "# Opening the URL \n",
    "#driver = uc.Chrome(options=options)\n",
    "driver = uc.Chrome()\n",
    "# url_list = []\n",
    "url = 'https://cmu.primo.exlibrisgroup.com/discovery/search?institution=01CMU_INST&vid=01CMU_INST:01CMU&tab=Everything&search_scope=MyInst_and_CI&mode=Basic&displayMode=full&bulkSize=10&highlight=true&displayField=all&query=any,contains,proquest&searchtext=proquest'\n",
    "# counter = 0\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70fe4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Login on your CMU Library System, Search for \"ProQuest,\" and open the database. Click on change databases and choose Indian Historical Database and Indian Newspaper - Times of India.\n",
    "## Within the advanced search box, enter the name of first singer celebrity manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5017d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure the driver is on the correct tab\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "page_html = driver.page_source\n",
    "soup = BeautifulSoup(page_html,'html.parser')\n",
    "#creating the dataframe\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e66d6318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Lata Mangeshkar')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(4)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_LataMangeshkar_2.csv\")\n",
    "print(\"Code Execution Finished Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f152fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Source</th>\n",
       "      <th>Link</th>\n",
       "      <th>Singer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Source, Link, Singer]\n",
       "Index: []"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f79f6341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Asha Bhosle')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(4)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_AshaBhosle.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f5f8e5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Pankaj Udhas')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(4)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_PankajUdhas.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0a09fa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Jagjit Singh')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(4)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_JagjitSingh.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78cebc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Kumar Sanu')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(4)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_KumarSanu.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5c4f7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Manna Dey')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(4)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_MannaDey.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d7278b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Shubha Mudgal')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(3)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_ShubhaMudgal.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "532c5aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Chitra Singh')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(3)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_ChitraSingh.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ec2295d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Birju Maharaj')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(3)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_BirjuMaharaj.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5a986f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Parsing through pages\n",
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Zoheb Hassan')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(3)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_ZohebHassan.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "075e9d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scraping for Singer Complete\n",
      "Code Execution Finished Successfully\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html,'html.parser')\n",
    "    \n",
    "    # Getting the table (ul tag)\n",
    "    table1 = soup.find_all('a', attrs={'class':'previewTitle Topicsresult'})\n",
    "    table2 = soup.find_all('span', attrs={'class':'newspaperArticle'})\n",
    "\n",
    "    #all the href tag values (links) are extracted\n",
    "    href = []\n",
    "    title = []\n",
    "    source = []\n",
    "    Singer = []\n",
    "\n",
    "    for link in table1:\n",
    "        href.append(link.get('href'))\n",
    "        title.append(link.get('title'))\n",
    "        Singer.append('Zoheb Hassan')\n",
    "        \n",
    "        \n",
    "\n",
    "    for text in table2:\n",
    "        source.append(text.find_all('span')[0].text)\n",
    "        \n",
    "\n",
    "    #saving the dataframe\n",
    "    df_temp = pd.DataFrame({'Title': title, 'Source': source, 'Link': href, 'Singer': Singer})\n",
    "    df_temp\n",
    "    df = pd.concat([df,df_temp])\n",
    "\n",
    "    #Clicking the Next Page Button until available\n",
    "    try:\n",
    "\n",
    "        next_page_button = driver.find_element(By.XPATH, \"/html/body/div[4]/div[1]/div/div[7]/div[7]/div[3]/div[1]/div[4]/div[3]/div/div/form/nav/ul/li[9]/a\")\n",
    "        next_page_button.click()\n",
    "        #Sleep option\n",
    "        print(\"Parsing through pages\")\n",
    "        time.sleep(3)\n",
    "        continue\n",
    "\n",
    "    except:\n",
    "        print(\"Data Scraping for Singer Complete\")\n",
    "        break\n",
    "\n",
    "df.to_csv(\"/Users/sachinsrivastava/Downloads/PythonCodes/SingersData03302023/Hass_DIL_Proquest_Singers_Data_ZohebHassan.csv\")\n",
    "print(\"Code Execution Finished Successfully\")\n",
    "df = df.drop(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2527a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e04b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
